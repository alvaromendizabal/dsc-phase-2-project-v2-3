{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_validate\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence as influence\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the file\n",
    "df = pd.read_csv('data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the first 5 entries in the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the columns and nulls\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for year renovated, convert any houses that have been renovated to '1' to indicate true\n",
    "#for any nulls, assume no renovation\n",
    "df['yr_renovated'].fillna(0, inplace=True)\n",
    "df['yr_renovated'] = df['yr_renovated'].apply(lambda x: 1 if x > 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'yr_renovated': 'if_renovated'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up sqft_basement and convert to int\n",
    "df['sqft_basement'] = df['sqft_basement'].replace({'?':np.nan}).astype(float)\n",
    "df['sqft_basement'].fillna(df['sqft_living']-df['sqft_above'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the months and year\n",
    "df['month_of_date'] = pd.DatetimeIndex(df['date']).month\n",
    "df['year_of_date'] = pd.DatetimeIndex(df['date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert yr_built to age of house by subtracting year the property was sold by the year it was built\n",
    "#to create a more sensible column \n",
    "df['age_of_house'] = df['year_of_date'] - df['yr_built']\n",
    "\n",
    "#drop year of date because years are only 2014 and 2015, and will not impact our predicative model\n",
    "#drop yr_built b/c it is redundant with age_of_house\n",
    "df.drop(columns=['year_of_date'], inplace=True)\n",
    "df.drop(columns=['yr_built'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates if any\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop id and date columns\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert some of the categorical data from floats to ints\n",
    "df['waterfront'] = df['waterfront'].astype(int)\n",
    "df['view'] = df['view'].astype(int)\n",
    "df['sqft_basement'] = df['sqft_basement'].astype(int)\n",
    "df['if_renovated'] = df['if_renovated'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check cleaned data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since price is our target, we will explore first\n",
    "#view distribution of price using histogram\n",
    "sns.set(style = 'white')\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "sns.histplot(data = df, x = 'price', palette = 'Dark', )\n",
    "ax.set_xlabel('Price')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a boxplot for price\n",
    "sns.set(style = 'white')\n",
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "sns.boxplot(data = df, x = 'price', palette = 'pastel', fliersize = 5, whis = 8)\n",
    "ax.set_xlabel('Price')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because the data is skewed to the right, transform the price data using log\n",
    "df['ln_price'] = np.log(df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view distribution of log base e for price using histogram\n",
    "sns.set(style = 'white')\n",
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "sns.histplot(data = df, x = 'ln_price', palette = 'Dark')\n",
    "ax.set_xlabel('Natural Log of Price')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Natural Log of Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'white')\n",
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "sns.boxplot(data = df, x = 'ln_price', palette = \"pastel\")\n",
    "ax.set_xlabel(\"Price\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Distribution of Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the pairplot, we can see which data are categorical and which are numeric\n",
    "numeric = ['bedrooms', \n",
    "           'bathrooms', \n",
    "           'sqft_living', \n",
    "           'sqft_lot', \n",
    "           'sqft_above', \n",
    "           'sqft_basement',\n",
    "           'lat', \n",
    "           'long',\n",
    "           'sqft_living15', \n",
    "           'sqft_lot15']\n",
    "\n",
    "categorical = ['floors',\n",
    "               'waterfront', \n",
    "               'view', \n",
    "               'condition', \n",
    "               'grade',\n",
    "               'if_renovated',\n",
    "               'zipcode',\n",
    "               'month_of_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#visually inspecting value counts to look for weird values\n",
    "for column in df.columns:\n",
    "    if not column == 'price':\n",
    "        display(df[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#found 1 obs that should be removed\n",
    "df[df['bedrooms'] >= 20]\n",
    "df.drop(15856, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these look fine\n",
    "df[df['bathrooms']>6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with the target as the first column,\n",
    "# then compute the correlation matrix\n",
    "X = df[numeric]\n",
    "y = df['price']\n",
    "heatmap_data = pd.concat([y, X], axis=1)\n",
    "corr = heatmap_data.corr()\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "# Plot a heatmap of the correlation matrix, with both\n",
    "# numbers and colors indicating the correlations\n",
    "sns.heatmap(\n",
    "    # Specifies the data to be plotted\n",
    "    data=corr,\n",
    "    # The mask means we only show half the values,\n",
    "    # instead of showing duplicates. It's optional.\n",
    "    mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "    # Specifies that we should use the existing axes\n",
    "    ax=ax,\n",
    "    # Specifies that we want labels, not just colors\n",
    "    annot=True,\n",
    "    # Customizes colorbar appearance\n",
    "    cbar_kws={\"label\": \"Correlation\", \"orientation\": \"horizontal\", \"pad\": .2, \"extend\": \"both\"}\n",
    ")\n",
    "\n",
    "# Customize the plot appearance\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes and Price\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reporting the correlation between price (target) and predictors\n",
    "df.corr()['price'].drop(['ln_price']).map(abs).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with the target as the first column,\n",
    "# then compute the correlation matrix\n",
    "X = df[numeric]\n",
    "ln_y = df['ln_price']\n",
    "heatmap_data = pd.concat([ln_y, X], axis=1)\n",
    "corr = heatmap_data.corr()\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "# Plot a heatmap of the correlation matrix, with both\n",
    "# numbers and colors indicating the correlations\n",
    "sns.heatmap(\n",
    "    # Specifies the data to be plotted\n",
    "    data=corr,\n",
    "    # The mask means we only show half the values,\n",
    "    # instead of showing duplicates. It's optional.\n",
    "    mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "    # Specifies that we should use the existing axes\n",
    "    ax=ax,\n",
    "    # Specifies that we want labels, not just colors\n",
    "    annot=True,\n",
    "    # Customizes colorbar appearance\n",
    "    cbar_kws={\"label\": \"Correlation\", \"orientation\": \"horizontal\", \"pad\": .2, \"extend\": \"both\"}\n",
    ")\n",
    "\n",
    "# Customize the plot appearance\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes and Price\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reporting the correlation between ln price (target) and predictors\n",
    "df.corr()['ln_price'].drop(['price']).map(abs).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the numeric features to see distribution and see if they need to be log-transformed\n",
    "df[numeric].hist(figsize=[6,6]);\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transform all numeric feature except:\n",
    "#sqft basement - has values of 0\n",
    "#long - has negative values\n",
    "to_ln = ['bedrooms',\n",
    "         'bathrooms',\n",
    "         'sqft_living',\n",
    "         'sqft_lot', \n",
    "         'sqft_above',\n",
    "         'lat',\n",
    "         'sqft_living15', \n",
    "         'sqft_lot15']\n",
    "\n",
    "for column in to_ln:\n",
    "    df[column] = np.log(df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mainipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two dataframes, one without ln_price, and without price\n",
    "output = df.drop(['ln_price'], axis=1) \n",
    "output_ln = df.drop(['price'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df, target, test_size=0.20, random_state=42):\n",
    "    '''\n",
    "    This function takes in a dataframe df and target column and returns the train and test split\n",
    "    Default test size is 20, default random state is 42\n",
    "    '''\n",
    "    \n",
    "    #dropping targets out of predictors\n",
    "    X = df.drop(target, axis=1)\n",
    "\n",
    "    #set target with y\n",
    "    y = df[target]\n",
    "    \n",
    "    #creating  train test split for model comparison\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    #instantiating OHE object\n",
    "    ohe = OneHotEncoder(sparse=False, handle_unknown='error', drop='first')\n",
    "\n",
    "    #Fitting object onto test and trasnforming test and train\n",
    "    X_train_ohe = ohe.fit_transform(X_train[categorical])\n",
    "    X_test_ohe = ohe.transform(X_test[categorical])\n",
    "\n",
    "    #placing column names onto our new categorical columns and formatting as df\n",
    "    X_train_ohe_df = pd.DataFrame(X_train_ohe, columns=ohe.get_feature_names(categorical), \n",
    "                              index=X_train.index)\n",
    "    X_test_ohe_df = pd.DataFrame(X_test_ohe, columns=ohe.get_feature_names(categorical),\n",
    "                            index=X_test.index)\n",
    "\n",
    "    #combining categoricals with rest of data\n",
    "    X_train = pd.concat([X_train[numeric], X_train_ohe_df],axis=1)\n",
    "    X_test = pd.concat([X_test[numeric], X_test_ohe_df], axis=1)\n",
    "\n",
    "    \n",
    "    X_list = [X_train, X_test]\n",
    "    \n",
    "    #scaling X values into z-scores\n",
    "    ss = StandardScaler()\n",
    "    for i in X_list:\n",
    "        ss.fit(i)\n",
    "        i = pd.DataFrame(ss.transform(i))\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Formulas and Useful Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(estimator,X,y,n_splits=10,test_size=0.25, random_state=None):\n",
    "    \"\"\"\n",
    "    This formula performs cross validation using shuffled splits. Output is a tuple,\n",
    "    The 0th element is the median R2 score for the train sets, the 1st element\n",
    "    is the median R2 score for the test sets.\n",
    "    \n",
    "    \"\"\"\n",
    "    splitter = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    output = cross_validate(estimator, X=X, y=y, cv=splitter, return_train_score=True)\n",
    "    return np.median(output['train_score']), np.median(output['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a summary of the median train R-squared, median test R-squared, and differential score based\n",
    "#on the cross validation\n",
    "def cval_summary(train,test,diff):\n",
    "    return f\"The median R-squared values for the train sets were {round(train,3)}, the median R-squared values for the test sets were {round(test,3)}. The"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, Xtrain, Xtest, ytrain, ytest, log=False):\n",
    "    '''\n",
    "    This function takes in a model and the train and test samples and returns\n",
    "    the train R-squared, test R-squared, and the RMSE\n",
    "    '''\n",
    "    if log == False:\n",
    "        rmse = mean_squared_error(ytest, model.predict(Xtest), squared=False)\n",
    "    else:\n",
    "        rmse = mean_squared_error(np.exp(ytest), np.exp(model.predict(Xtest)), squared=False)\n",
    "    return model.score(Xtrain, ytrain),  model.score(Xtest, ytest), rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a summary of the train R-squared, test R-squared, differential between R-squared, and RMSE\n",
    "def model_summary(train,test,diff,rmse):\n",
    "    return f\"The R-squared value for the train set was {round(train,3)}, and the R-squared value for the test set was {round(test,3)}. These values resulted in a differential of {round(diff,5)}. The RMSE of our model predicitons was {round(rmse,2)}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
